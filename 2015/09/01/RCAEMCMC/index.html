
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>RCAE &amp; MCMC | neuRonXe&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="前言第一次写这个文章的时候，是在读过了神网大神之一（确切来说是DL大神之一）——Yoshua Bengio还未出版的Deep Learning之后有感而发。自那以后到现在，已经整整半年，大神的草稿也已经成了……更长的草稿。当时读过的那一章，The Manifold Perspective on Auto-Encoders，在05/19的更新里已经被扩充成了三章： Linear Factor Mod">
<meta property="og:type" content="article">
<meta property="og:title" content="RCAE & MCMC">
<meta property="og:url" content="http://yoursite.com/2015/09/01/RCAEMCMC/index.html">
<meta property="og:site_name" content="neuRonXe's Blog">
<meta property="og:description" content="前言第一次写这个文章的时候，是在读过了神网大神之一（确切来说是DL大神之一）——Yoshua Bengio还未出版的Deep Learning之后有感而发。自那以后到现在，已经整整半年，大神的草稿也已经成了……更长的草稿。当时读过的那一章，The Manifold Perspective on Auto-Encoders，在05/19的更新里已经被扩充成了三章： Linear Factor Mod">
<meta property="og:image" content="https://drive.google.com/uc?export=view&id=0B9_Jak1oUX5GMFJJRG5lLWp5SGc">
<meta property="og:updated_time" content="2016-08-15T19:43:06.533Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RCAE & MCMC">
<meta name="twitter:description" content="前言第一次写这个文章的时候，是在读过了神网大神之一（确切来说是DL大神之一）——Yoshua Bengio还未出版的Deep Learning之后有感而发。自那以后到现在，已经整整半年，大神的草稿也已经成了……更长的草稿。当时读过的那一章，The Manifold Perspective on Auto-Encoders，在05/19的更新里已经被扩充成了三章： Linear Factor Mod">
  
    <link rel="alternative" href="/atom.xml" title="neuRonXe&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
</head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">neuRonXe&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">A neuron embedding in the infinite dimentional universe</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/About">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="yoursite.com">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main"><article id="post-RCAEMCMC" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/09/01/RCAEMCMC/" class="article-date">
  <time datetime="2015-09-01T15:59:44.000Z" itemprop="datePublished">2015-09-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      RCAE &amp; MCMC
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>第一次写这个文章的时候，是在读过了神网大神之一（确切来说是DL大神之一）——Yoshua Bengio还未出版的<a href="http://www.iro.umontreal.ca/~bengioy/dlbook/" target="_blank" rel="external"><strong>Deep Learning</strong></a>之后有感而发。自那以后到现在，已经整整半年，大神的草稿也已经成了……更长的草稿。当时读过的那一章，<strong>The Manifold Perspective on Auto-Encoders</strong>，在05/19的更新里已经被扩充成了三章： <strong>Linear Factor Models and Auto-Encoders</strong>，<strong>Representation Learning</strong>和<strong>The Manifold Perspective on Representation Learning</strong>。最近闲下来，决定重温这几章的内容，也学着大神把原来写的读后感扩充一下。嗯！</p>
<a id="more"></a>
<h2 id="1_Regularized_AutoEncoder__u4E0E_Manifold_Learning"><a href="#1_Regularized_AutoEncoder__u4E0E_Manifold_Learning" class="headerlink" title="1 Regularized AutoEncoder 与 Manifold Learning"></a>1 Regularized AutoEncoder 与 Manifold Learning</h2><p>自本挫接触DL以来，对DL的各种非监督学习算法就一直非常感兴趣。尤其是DL大行其道之后，感觉DL的特征学习的精髓都在的理解总是差那么一点点，于是这几天抽空把这一章读完，顺便读了两个相关的论文，感受颇深。虽然是草稿，但读过之后确实对自编码器和流形学习的关系“浮想联翩”了。因此，今天决定在这里记录一些自己的理解。</p>
<p>说到深度学习，就不得不提到自编码器。在非常有名的Stanford深度学习教程UFLDL中，深度神经网络的建模过程可以被看做：训练AE $\rightarrow$ 堆叠AE $\rightarrow$ 链接分类/回归器。因此，AE的知名度似乎要远大于比它的“前辈”RBM（也可能是因为RBM需要Markov网络的知识）。在UFLDL中，Ng大神主要讲述了一种常见的AE——Sparse AE（其实包括Bottleneck AE）。除了SAE之外，学者们还先后提出了Denoising AE和 Contractive AE。实际上，这些都可以被归并为Regularized AE，即对AE进行限制，迫使它们学习到输入数据的特征。SAE是利用稀疏惩罚，DAE利用了类似Dropout的办法，CAE则是惩罚AE编码部分的一阶导数（由于解码部分用了同样的权重$W$，实际上也算是对整体的一阶导数加罚了）。总之，Regularized AE的特点就一个字：Penalty！罚！</p>
<p>为什么罚？YB大神在DAE的文章中已经给出了关于DAE的流形学习解释：由于流行假设，数据是嵌在高维空间的低维流形。举例来说，虽然一个圆柱形的罐子曲面上的每一个点都处于一个三维空间，但我们只要知道了它在曲面上的二维坐标（局部的切平面）就可表示它了。流形学习就是基于这个假设，认为大多数自然中的数据，例如图像、语音都是符合这个规律。而罚AE的方法，就是强制告诉AE，你只要几个维度就可以完美表示得了这些高维数据的规律。例如DAE的通过对破坏数据还原的学习，能够把数据从高维空间降到低维空间加以表示，从而学习到数据的生成规律（分布）。这来源于训练AE时的两方面力量：</p>
<ul>
<li><p><strong>1 Reconstruction error</strong> 它要求AE的训练结果尽可能复原输入。</p>
</li>
<li><p><strong>2 Regularization</strong> 它要求AE的训练结果尽量少地对样本的无关变动敏感。</p>
</li>
</ul>
<p>这两个R的合力，就使得AE既要能够尽量还原输入，又要对无关变动不敏感。而符合这个要求的最优解，就是在数据集中的流形上面。一句话，RAE就是让AE学习到流形的形状，而为了这个目的，罚是必须的，否则就会让AE完美通过所有点，这样的解是没有任何意义的。（从这个角度看来，降维的方法都是在做这件事。例如PCA就是把数据从高维降到了线性的直线或者平面、超平面上。而AE是降到了非线性的超平面上）</p>
<h2 id="2_Denoising_AE__u4E0E_Contractive_AE"><a href="#2_Denoising_AE__u4E0E_Contractive_AE" class="headerlink" title="2 Denoising AE 与 Contractive AE"></a>2 Denoising AE 与 Contractive AE</h2><p>DAE的损失函数为：<br>$$L_{DAE}=||r(\tilde{x})-x||^2$$<br>其中$\tilde{x}$是通过一个方差为$\sigma^2$的Corruption分布将原样本破坏以后得到的新样本。正如前面提到的，DAE就是要从这些样本来试图还原本来的样本。</p>
<p>这里给出一个形象的解释，DAE的任务是要学习到一个统一的还原规律，使得对每一个输入$\tilde{x}$，还原成$x$的程度达到最大。因为每个样本加入的噪声都是随机的，那么完美还原就不太可能。于是，DAE就要寻找一个折中的办法，也就是还原到所有$x$对应的$E(x)$。如果所有的$x$都符合流形假设，那么很显然DAE就会选择让$\tilde{x}$还原到这个流形上对应的$x$。丢掉与流形不向切的其他方向，从而保证$x$的大部分特征能够还原。换句话说，通过加入噪声，使得DAE通过压缩噪声来达到正则化，学到了流形。</p>
<p>比起DAE，CAE采用的方式是直接对编码函数的一阶导数加罚：<br>$$L_{CAE}=||r(x)-x||^2+\lambda||\frac{\partial{f(x)}}{\partial{x}}||^2$$<br>保证学习到的编码函数，对输入的变动反应尽量小，即“收缩”。因此，CAE一方面要保留一些样本变动的方向来保证拟合，另一方面又要丢掉一些不重要的方向来保证罚。因此和DAE一样，CAE也会选择流形作为最优解。</p>
<p>综上，CAE和DAE确实都是在用不同的方法做类似的事情。但实际上不止如此，DAE和CAE之间还有更深层次的联系。YB大神在一篇论文中给出了这种关系。将DAE的损失函数变形，就可以写成如下形式：<br>$$E(L_{DAE})=||r(x)-x||^2+\sigma^2||\frac{\partial{r(x)}}{\partial{x}}||^2$$<br>详细推导过程见论文（基本的思路就是Taylor展开）。由此可见，DAE和CAE都可以看做给AE本身的导数加罚，DAE对整体加罚，而CAE只对编码部分加罚。并且DAE的参数和CAE的参数也是对应的。</p>
<p>得到了这些，YB大神干脆提出了一个General的模型：Reconstruction CAE，即CAE的整体加罚版本：<br>$$L_{RCAE}=||r(x)-x||^2+\lambda||\frac{\partial{r(x)}}{\partial{x}}||^2$$<br>这样，CAE和DAE就可以看做一类模型一起讨论了。</p>
<h2 id="3__u5BC6_u5EA6_u8F68_u70B9_u4E0E_u4E24_u79CDMCMC"><a href="#3__u5BC6_u5EA6_u8F68_u70B9_u4E0E_u4E24_u79CDMCMC" class="headerlink" title="3 密度轨点与两种MCMC"></a>3 密度轨点与两种MCMC</h2><p>前面鬼扯了一大堆，都是理论概念的理解，真正厉害的地方要从这里开始。</p>
<p>我们都知道RBM是一种概率模型，即它的推断和估计都必须通过抽样。虽然这给模型的解释性造成了困难，但如果学习目标就是一个概率分布，RBM理论上是可以拟合任意分布的。而反观AE，它是确定性的模型，解释起来要方便得多，但如果我们没办法从学到的分布中取样。而YB大神的另一个重要结论就是，DAE学习到的，其实就是生成样本的对数概率密度的导数：</p>
<p>$$r(x)=x+\sigma^2\frac{\partial\mathrm{log}(x)}{\partial x}+o(\sigma^2) \rightarrow \frac{r(x)-x}{\sigma^2}=\frac{\partial\mathrm{log}(x)}{\partial x}$$</p>
<p>这也就是说，通过充分训练的DAE，我们可以精确估计对数概率密度的导数。也就是小标题说的”轨点“了（轨点是梯度的英文音译，感觉和绩点GPA一样……出国党伤不起，这都能联系起来）。这就和MCMC联系了起来。这里YB大神还画了一个Vector Fields的图，训练的样本数据来自经典的Swiss Roll，训练的结果就好像attractor吸引子一样，无数向量都指向Swiss Roll，感觉非常漂亮！</p>
<p><img src="https://drive.google.com/uc?export=view&amp;id=0B9_Jak1oUX5GMFJJRG5lLWp5SGc" alt="Vector Fields of Denoising Autoencoder"></p>
<p>回忆经典的MH算法：根据细致平稳条件，假设我们能得到密度函数的比值$\frac{p(x^s)}{p(x)}$，再利用一个对称的预选分布$q(x^s|x)$，我们能利用MH算法取得$p(x)$的样本。换句话说，我们的任务就是设法得到$\frac{p(x^s)}{p(x)}$。下面详细讨论这一个问题：</p>
<p>$\frac{p(x^s)}{p(x)}$可以写成$e^{\mathrm{log}p(x^s)-\mathrm{log}p(x)}$，而 $\mathrm{log}p(x^s)-\mathrm{log}p(x)$ 进行一阶泰勒展开得到：<br>$$\mathrm{log}p(x^s)-\mathrm{log}p(x)=\frac{\partial\mathrm{log}(x)}{\partial x}(x^s-x)+o(x^s-x)$$</p>
<p>根据前面的结论我们知道利用DAE可以计算导数。因此，DAE可以根据这个方法构造MH（或者用Hamilton的方法构造HMC）抽样。</p>
<p>那么问题来了：标题中的“两种”作何解释？我们知道MCMC的方法主要有两类，MH和Gibbs。因此这个第二种就是Gibbs了。具体的方法也很简单：</p>
<p>因为我们有：<br>$$P(x,\tilde{x})=P(x)C(\tilde{x}|x)$$<br>其中C分布是已知的破坏分布。而根据DAE的训练方法我们知道，DAE可以从$\tilde{x}$还原$x$，说明DAE估计的结果就是$E(x|\tilde{x})$，也就是样本分布的均值。如果我们知道样本的具体分布，就可以得到$P(x|\tilde{x})$。而如果假定正态分布，我们只要计算一下样本的reconstruction error得到分布的另一个参数方差，就可以得到整体的分布了。</p>
<p>得到了我们想要的一切，就可以进行Gibbs抽样：</p>
<ul>
<li>1 $\tilde{X}_t=C(\tilde{X}|X_t)$</li>
<li>2 $X_{t+1}=P(X|\tilde{X}_t)$</li>
</ul>
<p>经过一段时间burn in，分布达到平稳，我们就可以去收人头……不对，是收$X$的样本了。由于是Gibbs，抽样的速度会快很多。</p>
<h2 id="4__u603B_u7ED3"><a href="#4__u603B_u7ED3" class="headerlink" title="4 总结"></a>4 总结</h2><p>大致总结起来就两句话：自编码器加罚来得到流形。通过加入随机性来进行抽样。看完这些后感觉AE现在的解释力度（特征提取）和实用性（抽样）都已经很强大，但YB大神似乎没有满足，干脆提出了个Generative Stochastic Network，把随机性引入了DAE的任一节点，不明觉厉。不过话说回来，我看这些的目的可不只是睡前读物……对付高频金融的第二个难题，DAE，就就决定是你了！</p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://yoursite.com/2015/09/01/RCAEMCMC/" data-id="cirwgpld800013kv2pzlg99ju" class="article-share-link" data-share="baidu" data-title="RCAE &amp; MCMC">Share</a>
      

      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/">ML</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/01/09/Deep-Learning-Deep-Regularization/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Deep Learning, Deep Regularization?
        
      </div>
    </a>
  
  
    <a href="/2015/06/06/Hello-Hexo/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello Hexo</div>
    </a>
  
</nav>

  
</article>

</section>
      
      <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS/">CS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/">DL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Intro/">Intro</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/">ML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Manual/">Manual</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/学术/">学术</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/DL/" style="font-size: 10px;">DL</a> <a href="/tags/Intro/" style="font-size: 10px;">Intro</a> <a href="/tags/ML/" style="font-size: 10px;">ML</a> <a href="/tags/Manual/" style="font-size: 10px;">Manual</a> <a href="/tags/学术/" style="font-size: 10px;">学术</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/01/09/Deep-Learning-Deep-Regularization/">Deep Learning, Deep Regularization?</a>
          </li>
        
          <li>
            <a href="/2015/09/01/RCAEMCMC/">RCAE &amp; MCMC</a>
          </li>
        
          <li>
            <a href="/2015/06/06/Hello-Hexo/">Hello Hexo</a>
          </li>
        
          <li>
            <a href="/2015/06/05/From-GitBook-to-Hexo-新的开始/">From GitBook to Hexo: 新的开始</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="http://xiguabaobao.com" target="_blank">主题作者</a>
          </li>
        
          <li>
            <a href="http://reqianduan.com" target="_blank">热前端</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 neuRonXe&#39;s Blog<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/About" class="mobile-nav-link">About</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="http://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script src="/js/script.js" type="text/javascript"></script>

</div>
</body>
</html>
